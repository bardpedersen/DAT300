{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voluntary Assignment\n",
    "\n",
    "The aim of this assignments is to put the basic theory of artificial neural networks into practice, and get an idea of what the structure of compulsory assignments looks like. To complete the assignment, please read the assignment text carefully and fill out the appropriate cells. \n",
    "\n",
    "# Introduction \n",
    "\n",
    "In this assignment you will implement a neural network with the numpy package in Python and train it for the Handwritten Digits recognition problem. The dataset is comprised of a 60,000 greyscale images with $(28 \\times 28)$ pixels. Each image contains a handwritten digit collected by the National Institute of Standards and Technology. The images have already been normalized, and centered so there is no need for preprocessing. \n",
    "\n",
    "<center><img src=\"images/mnist_white.jpeg\" width=\"200\" height=\"200\"><img src=\"images/mnist_black.jpeg\" width=\"200\" height=\"200\"></center>\n",
    "\n",
    "Some code will be provided for you, but you will be asked to implement some functions your self too.\n",
    "The function *load_data()* in the file [utilities.py](./utilities.py) will download the MNIST dataset the first time you run it, and simply load it from disk after that. \n",
    "\n",
    "# Imports\n",
    "\n",
    "In the code cell below, all the neccesary packages are imported, and the dataset is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from utilities import *\n",
    "from tqdm import tqdm # Cool progress bar\n",
    "import matplotlib.pyplot as plt\n",
    "from utilities import load_mnist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 458\n",
    "RNG = np.random.default_rng(SEED) # Random number generator\n",
    "\n",
    "datasets = load_mnist()\n",
    "X_train       = datasets['X_train']\n",
    "y_train       = datasets['y_train']\n",
    "X_val         = datasets['X_val']\n",
    "y_val         = datasets['y_val']\n",
    "X_test        = datasets['X_test']\n",
    "y_test        = datasets['y_test']\n",
    "small_X_train = datasets['small_X_train']\n",
    "small_y_train = datasets['small_y_train']\n",
    "small_X_val   = datasets['small_X_val']\n",
    "small_y_val   = datasets['small_y_val']\n",
    "del datasets # Good to reduce uneccesary RAM usage\n",
    "\n",
    "# Normalizing datasets between [0,1]\n",
    "X_train       = X_train.astype(\"float32\")      /np.max(X_train)\n",
    "X_val         = X_val.astype(\"float32\")        /np.max(X_val)\n",
    "X_test        = X_test.astype(\"float32\")       /np.max(X_test)\n",
    "small_X_train = small_X_train.astype(\"float32\")/np.max(small_X_train)\n",
    "small_X_val   = small_X_val.astype(\"float32\")  /np.max(small_X_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "Below is a plot visualizing some examples of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images = RNG.choice(X_train, replace=False, size=4)\n",
    "fig, ax = plt.subplots(1,4,figsize=(8,8))\n",
    "for i in range(random_images.shape[0]):\n",
    "    ax[i].imshow(random_images[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Compute the derivatives of activation functions\n",
    "\n",
    "In this task we will consider twe famous activation functions. The sigmoid function and the hyperbolic tangent function. The formulas for each activation function are given below: \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{sigmoid}(x) = \\frac{1}{1 + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathrm{tanh}(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}\n",
    "\\end{equation}\n",
    "\n",
    "The code implementations for each activation function is given below. The first task is to implement the derivatives of the individual activation functions in the function-shells given. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def D_sigmoid(x):\n",
    "    ...\n",
    "\n",
    "def D_tanh(x):\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Design your very own neural network\n",
    "\n",
    "In the code cell below, a shell-class has been provided for an artificial neural network implemented in numpy. The ANN will always have one input layer, one hidden layer and one output layer. But, the number of neurons in each layer is not fixed. Most of the code for the ANN is implemented, but two functions are missing: the *forward_propagation* and the *back_propagation* member functions. **Your task is to implement them**.\n",
    "\n",
    "## Forward propagation\n",
    "\n",
    "The network has been implemented with the following notation in mind. Let $X$ be the input data and $\\hat{y}$ be the output data. Let the weights from the input layer to the hidden layer be denoted $\\alpha$ and the weights from the hidden layer to the output layer be denoted $\\beta$. We then have that the $i$'th neuron in the hidden layer will have the pre-activation output $u^{(hidden)}_i$ given by\n",
    "\n",
    "$$\n",
    "u^{(h)}_i = \\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\n",
    "$$\n",
    "\n",
    "Where $\\alpha_i^{(b)}$ is the bias, and $\\alpha_i^{(j)}$ are the regular weights. The post-activation output of neuron $i$ in the hidden layer, $o^{(hidden)}_i$, is then given by\n",
    "\n",
    "\\begin{align*}\n",
    "o^{(h)}_i &= f(u^{(h)}_i) \\\\ \n",
    "          &= f\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right)\n",
    "\\end{align*}\n",
    "\n",
    "The pre-activation output of neuron $k$ in the output layer, $u^{(output)}_i$, is given by:\n",
    "\n",
    "\\begin{align*}\n",
    "u^{(o)}_k &= \\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i \\\\ \n",
    "          &= \\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k f\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\beta^{(b)}_k$ is the bias, and $\\beta_k^{(j)}$ are the regular weights. The post-activation output of neuron $k$ in the output layer, $o^{(output)}_k$, is then given by\n",
    "\n",
    "\\begin{align*}\n",
    "o^{(o)}_k &= f(u^{(o)}_k) \\\\\n",
    "          &= f\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\\\\n",
    "          &= f\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k f\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "### FP implementation tips\n",
    "These formulas should make it easier to implement the forward propagation function. But, be aware that the inputs might come in batches. So make use of the vector multiplication functions in numpy such as: \n",
    "```python\n",
    "# Elementwise multiplication of vectors\n",
    "np.multiply(X,y) \n",
    "X*y \n",
    "# Dot product\n",
    "np.dot(X,y)\n",
    "X@y \n",
    "# Transpose\n",
    "np.transpose(X)\n",
    "X.T\n",
    "```\n",
    "Another tip is to try and break up the forward propagation function into the steps $u^{(h)}_i$, $o^{(h)}_i$, $u^{(o)}_k$ and $o^{(o)}_k$ and store them as member-variables of the network class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "### Theoretical derivation (you can skip this subsection if it is too heavy)\n",
    "\n",
    "Back propagation is where the weights of the network are adjusted according to how well they are able to estimate the target. To be able to perform back propagation we first must define the loss function. For this network Mean Square Error is used as loss, meaning that the loss function is given by\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{B}(\\overrightarrow{y} - \\overrightarrow{o^{(o)}})^2\n",
    "$$\n",
    "\n",
    "Where $\\frac{1}{B}$ is the batch size, $\\overrightarrow{y}$ is the target, and $\\overrightarrow{o^{(o)}}$ is the final output from the output layer. To perform backpropagation we are going to use the gradient descent algorithm which is an iterative optimization algorithm that is used to minimize the loss function. Gradient descent relies on the updating the individual weights with an amount proportional to the partial derivative of the loss function with respect to the individual weights. Let $\\theta^{(t)}$ be an arbitrary weight of the network at iteration, the weight update is then given by: \n",
    "\n",
    "$$\n",
    "\\theta^{(t + 1)} = \\theta^{(t)} + \\gamma \\frac{\\partial L}{\\partial \\theta}_{\\theta = \\theta^{(t)}}\n",
    "$$\n",
    "\n",
    "Where $\\gamma$ is called the _learning rate_. The hardest part of implementing gradient descent with back propagation is to compute the partial derivative of the loss function with respect to the individual weights. To do this we utilize the chain rule of differentiation, since an ANN is basically a composite chain of weighted sums and activation functions. \n",
    "\n",
    "\\begin{align*}\n",
    "L &= \\frac{1}{B}(\\overrightarrow{y} - \\overrightarrow{o^{(o)}})^2 \\\\\n",
    "L &= \\frac{1}{B}(\\overrightarrow{y} - f\\left(\\beta \\times f\\left(\\alpha \\times X\\right)\\right))^2\n",
    "\\end{align*}\n",
    "\n",
    "Let's start with computing the partial derivative of $L$ with respect to the bias weight between the hidden layer and the output layer ($\\beta^{(b)}_k$), using the chain rule we get that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta^{(b)}_k} = \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(b)}_k}\n",
    "$$\n",
    "\n",
    "This yields us two other partial derivative terms that are easier to calculate. First we start with $\\frac{\\partial L}{\\partial o^{(o)}_k}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial o^{(o)}_k} &= \\frac{\\partial}{\\partial o^{(o)}_k}\\left[ (\\overrightarrow{y} - \\overrightarrow{o^{(o)}})^2 \\right] \\\\\n",
    "                                      &= -2\\frac{1}{B}\\times\\left(y_k - o^{(o)}_k\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Then we continue with $\\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(b)}_k}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(b)}_k} &= \\frac{\\partial}{\\partial \\beta^{(b)}_k} \\left[ f\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\right] \\\\\n",
    "                                                  &= f'\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\\\\n",
    "                                                  &= f'\\left(u^{(o)}_k\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Yielding a final expression for $\\frac{\\partial L}{\\partial \\beta^{(b)}_k}$. Now continuing with $\\frac{\\partial L}{\\partial \\beta^{(i)}_k}$, again we use the chain rule to split the derivative into multiple terms that are easier to compute and store. \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\beta^{(i)}_k} = \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(i)}_k}\n",
    "\\end{align*}\n",
    "\n",
    "The derivative $\\frac{\\partial L}{\\partial o^{(o)}_k}$ was calculated in the previous step, so we derive $\\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(i)}_k}$. \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(i)}_k} &= \\frac{\\partial}{\\partial \\beta^{(i)}_k} \\left[ f\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\right] \\\\\n",
    "                                                  &= f'\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) o^{(h)}_i \\\\\n",
    "                                                  &= f'\\left( u^{(o)}_k \\right) o^{(h)}_i\n",
    "\\end{align*}\n",
    "\n",
    "Yielding a final expression for $\\frac{\\partial L}{\\partial \\beta^{(i)}_k}$. Now continuing with $\\frac{\\partial L}{\\partial \\alpha^{(b)}_i}$, again we use the chain rule to split the derivative into multiple terms that are easier to compute and store.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\alpha^{(b)}_i} = \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i} \\times \\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(b)}_i}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, we must compute $\\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i}$, and $\\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(b)}_i}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i} &= \\frac{\\partial}{\\partial o^{(h)}_i} \\left[ f\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\right] \\\\\n",
    "                                              &= \\beta^{(i)}_k f'\\left(\\beta^{(b)}_k + \\sum_{i = 1}^{\\mathrm{hidden}\\:\\mathrm{neurons}} \\beta^{(i)}_k o^{(h)}_i\\right) \\\\\n",
    "                                              &= \\beta^{(i)}_k f'\\left(u^{(o)}_k \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(b)}_i} &= \\frac{\\partial}{\\partial \\alpha^{(b)}_i} \\left[ f\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right) \\right] \\\\\n",
    "                                                   &= f'\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right) \\\\\n",
    "                                                   &= f'\\left(u^{(h)}_i\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Yielding a final expression for $\\frac{\\partial L}{\\partial \\alpha^{(b)}_i}$. Now continuing with $\\frac{\\partial L}{\\partial \\alpha^{(j)}_i}$, again we use the chain rule to split the derivative into multiple terms that are easier to compute and store.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\alpha^{(j)}_i} = \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i} \\times \\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(j)}_i}\n",
    "\\end{align*}\n",
    "\n",
    "Thus, what remains is only to compute $\\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(j)}_i}$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(j)}_i} &= \\frac{\\partial}{\\partial \\alpha^{(j)}_i} \\left[ f\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right) \\right] \\\\\n",
    "                                                   &= x_j f'\\left(\\alpha_i^{(b)} + \\sum_{j = 1}^{\\mathrm{input}\\:\\mathrm{size}} \\alpha_i^{(j)} x_j\\right) \\\\\n",
    "                                                   &= x_j f'\\left(u^{(h)}_i\\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "In our implementation of the neural network we can discard the formulas for updating the two bias weight terms ($\\alpha^{(b)}_i$, $\\beta^{(b)}_k$) and will account for the bias by padding $X$ and $o^{(h)}$ with an additional row of ones. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BP implementation tips\n",
    "\n",
    "Now you have all the formulas you need to implement estimators for\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial \\alpha^{(j)}_i} \\\\\n",
    "\\frac{\\partial L}{\\partial \\beta^{(i)}_k}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "and use them to update the weight terms using \n",
    "\n",
    "$$\n",
    "\\theta^{(t + 1)} = \\theta^{(t)} + \\gamma \\frac{\\partial L}{\\partial \\theta}_{\\theta = \\theta^{(t)}}\n",
    "$$\n",
    "\n",
    "The same tips given for the forward propagation section holds for this section as well. Try to account for vectorized operations, and implement the expressions for finding the following values in the following order:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial o^{(o)}_k}              &= -2\\frac{1}{B}\\times\\left(y_k - o^{(o)}_k\\right) \\\\\n",
    "\\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(i)}_k}  &= o^{(h)}_i \\times f'\\left( u^{(o)}_k \\right) \\\\\n",
    "\\frac{\\partial L}{\\partial \\beta^{(i)}_k}          &= \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial \\beta^{(i)}_k} \\\\\n",
    "\\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i}      &= \\beta^{(i)}_k \\times f'\\left(u^{(o)}_k \\right) \\\\\n",
    "\\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(j)}_i} &= x_j \\times f'\\left(u^{(h)}_i\\right) \\\\\n",
    "\\frac{\\partial L}{\\partial \\alpha^{(j)}_i}         &= \\frac{\\partial L}{\\partial o^{(o)}_k} \\times \\frac{\\partial o^{(o)}_k}{\\partial o^{(h)}_i} \\times \\frac{\\partial o^{(h)}_i}{\\partial \\alpha^{(j)}_i}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_layer_size, hidden_layer_size, output_layer_size, transfer_f, transfer_df):\n",
    "        \"\"\"\n",
    "        input_layer_size : number of input neurons\n",
    "        hidden_layer_size: number of hidden neurons\n",
    "        output_layer_size: number of output neurons\n",
    "        \"\"\"\n",
    "        # Initialize transfer functions\n",
    "        self.transfer_f = transfer_f\n",
    "        self.transfer_df = transfer_df\n",
    "\n",
    "        # Initialize layer sizes\n",
    "        self.input_layer_size = input_layer_size + 1  \n",
    "        self.hidden_layer_size = hidden_layer_size + 1\n",
    "        self.output_layer_size = output_layer_size\n",
    "\n",
    "        # Initialize array for inputs\n",
    "        self.input = np.ones((1, self.input_layer_size))\n",
    "\n",
    "        # Initialize arrays for sums, and activations\n",
    "        self.u_hidden = np.zeros((1, self.hidden_layer_size - 1)) # Output of weighted sum in hidden layer\n",
    "        self.o_hidden = np.ones((1, self.hidden_layer_size))      # Output of activation in hidden layer\n",
    "        self.u_output = np.zeros((1, self.output_layer_size))     # Output of weighted sum in output layer\n",
    "        self.o_output = np.ones((1, self.output_layer_size))      # Output of activation in output layer\n",
    "\n",
    "        # Initialize partial derivatives\n",
    "        self.dL_do_output           = np.zeros((1, output_layer_size))\n",
    "        self.dL_dW_hidden_to_output = np.zeros((self.hidden_layer_size, self.output_layer_size))\n",
    "        self.dL_do_hidden           = np.zeros((1, self.hidden_layer_size))\n",
    "        self.dL_dW_input_to_hidden  = np.zeros((self.input_layer_size, self.hidden_layer_size))\n",
    "\n",
    "        # Initialize weights\n",
    "        self.re_initialize_weights()\n",
    "\n",
    "    def re_initialize_weights(self):\n",
    "        # Initialize arrays for weights\n",
    "        # Initializing randomly according to Yann LeCun's method in 1988 paper\n",
    "        input_range = 1.0 / self.input_layer_size ** (1/2)\n",
    "        self.W_input_to_hidden  = RNG.normal(loc=0, scale=input_range, size=(self.input_layer_size, self.hidden_layer_size - 1))\n",
    "        self.W_hidden_to_output = RNG.uniform(size=(self.hidden_layer_size, self.output_layer_size)) / np.sqrt(self.hidden_layer_size)\n",
    "\n",
    "        # Empty training logs\n",
    "        self.train_losses          = []\n",
    "        self.validation_losses     = []\n",
    "        self.training_accuracies   = []\n",
    "        self.validation_accuracies = []\n",
    "\n",
    "    def forward_propagation(self, inputs):\n",
    "        ## Set input\n",
    "        flattened_inputs = inputs.reshape(inputs.shape[0], -1)\n",
    "        # Padding input with 1s for adding bias\n",
    "        self.input = np.array([list(x) + [1.0] for x in flattened_inputs]) # Shape: (batch_size, number_of_input_values + 1)\n",
    "\n",
    "        # FILL INN YOUR IMPLEMENTATION OF FP HERE.\n",
    "\n",
    "    def back_propagation(self, targets, learning_rate=1.0):\n",
    "        ## Transforming the targets from scalar to softmax vector\n",
    "        softmax_targets = np.array([[1 if i == t else 0 for i in range(self.output_layer_size)] for t in targets]) \n",
    "        \n",
    "        # FILL INN YOUR IMPLEMENTATION OF BP HERE.\n",
    "\n",
    "    def train(self, X_t, y_t, X_v, y_v, epochs=25, learning_rate=5.0):\n",
    "        \"\"\"\n",
    "        X_t          : X train\n",
    "        y_t          : y train\n",
    "        X_v          : X validation\n",
    "        y_v          : y validation\n",
    "        epochs       : number of epochs\n",
    "        learning_rate: initial learning rate\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        for it in tqdm(range(epochs)):\n",
    "            self.forward_propagation(X_t)\n",
    "            self.back_propagation(y_t, learning_rate=learning_rate)\n",
    "\n",
    "            train_loss          = self.evaluate_loss(X_t, y_t)\n",
    "            validation_loss     = self.evaluate_loss(X_v, y_v)\n",
    "            training_accuracy   = self.evaluate_accuracy(X_t, y_t)\n",
    "            validation_accuracy = self.evaluate_accuracy(X_v, y_v)\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.validation_losses.append(validation_loss)\n",
    "            self.training_accuracies.append(training_accuracy)\n",
    "            self.validation_accuracies.append(validation_accuracy)\n",
    "\n",
    "        print(\"Training time:\", time.time()-start_time)\n",
    "        history = {\n",
    "            'train_loss': self.train_losses,\n",
    "            'validation_loss': self.validation_losses,\n",
    "            'training_accuracy': self.training_accuracies,\n",
    "            'validation_accuracy': self.validation_accuracies,\n",
    "        }\n",
    "        return history\n",
    "\n",
    "    def plot_training(self):\n",
    "        _, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "        ax[0].plot(np.array(self.train_losses), label='Training')\n",
    "        ax[0].plot(np.array(self.validation_losses), label='Validation')\n",
    "        ax[0].set_xlabel('Epoch')\n",
    "        ax[0].set_ylabel('Sum square error')\n",
    "        ax[0].set_title('Loss')\n",
    "\n",
    "        ax[1].plot(np.array(self.training_accuracies), label='Training')\n",
    "        ax[1].plot(np.array(self.validation_accuracies), label='Validation')\n",
    "        ax[1].set_xlabel('Epoch')\n",
    "        ax[1].set_ylabel('Accuracy [%]')\n",
    "        ax[1].set_title('Accuracy')\n",
    "\n",
    "        ax[0].grid('on')\n",
    "        ax[1].grid('on')\n",
    "\n",
    "        ax[0].legend()\n",
    "        ax[1].legend()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_loss(self, X, y):\n",
    "        self.forward_propagation(X)\n",
    "        prediction = np.argmax(self.o_output, axis=1)\n",
    "        return np.sum((y - prediction)**2)\n",
    "\n",
    "    def evaluate_accuracy(self, X, y):\n",
    "        self.forward_propagation(X)\n",
    "        predictions = np.argmax(self.o_output, axis=1)\n",
    "        count = len(X) - np.count_nonzero(y - predictions)\n",
    "        return 100*count/len(X) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Test you network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_neurons  = 28*28 # Image dimensions\n",
    "hidden_layer_neurons = 30\n",
    "outut_layer_neurons  = 10    # Number of digits to be recognized\n",
    "epochs               = 15\n",
    "lr                   = 0.25\n",
    "mnist_model = NeuralNetwork(input_layer_neurons, hidden_layer_neurons, outut_layer_neurons, sigmoid, D_sigmoid)\n",
    "\n",
    "history = mnist_model.train(small_X_train, small_y_train, small_X_val, small_y_val, epochs=epochs, learning_rate=lr)\n",
    "# In the other compulsory assignments you will not submit notebooks with the training logs\n",
    "mnist_model.plot_training()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Experiment with different learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_neurons  = 28*28 # Image dimensions\n",
    "hidden_layer_neurons = 45\n",
    "outut_layer_neurons  = 10    # Number of digits to be recognized\n",
    "epochs               = 15\n",
    "lrs                  = [1, 0.7, 0.5, 0.3, 0.2]\n",
    "mnist_model = NeuralNetwork(input_layer_neurons, hidden_layer_neurons, outut_layer_neurons, sigmoid, D_sigmoid)\n",
    "\n",
    "histories = []\n",
    "for lr in lrs:\n",
    "    mnist_model.re_initialize_weights()\n",
    "    hist =  mnist_model.train(small_X_train, small_y_train, small_X_val, small_y_val, epochs=epochs, learning_rate=lr)\n",
    "    # In the other compulsory assignments you will not submit notebooks with the training logs\n",
    "    histories.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax[0].set_title('Training Accuracy')\n",
    "ax[1].set_title('Validation Accuracy')\n",
    "\n",
    "for idx, lr in enumerate(lrs):\n",
    "    history = histories[idx]\n",
    "    ax[0].plot(np.array(history['training_accuracy']), label='lr=%.2f'%lr)\n",
    "    ax[1].plot(np.array(history['validation_accuracy']), label='lr=%.2f'%lr)\n",
    "\n",
    "for axes in ax:\n",
    "    axes.set_xlabel('Epoch [%]')\n",
    "    axes.set_ylabel('Accuracy')\n",
    "    axes.grid('on')\n",
    "    axes.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Experiment with different number of neurons in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_neurons  = 28*28 # Image dimensions\n",
    "hidden_layer_neurons = [30, 45, 60, 75]\n",
    "outut_layer_neurons  = 10    # Number of digits to be recognized\n",
    "epochs               = 15\n",
    "lr                   = 0.3\n",
    "\n",
    "histories = []\n",
    "for neur in hidden_layer_neurons:\n",
    "    mnist_model = NeuralNetwork(input_layer_neurons, \n",
    "                                neur, \n",
    "                                outut_layer_neurons, \n",
    "                                sigmoid, \n",
    "                                D_sigmoid)\n",
    "    hist =  mnist_model.train(small_X_train, small_y_train, small_X_val, small_y_val, epochs=epochs, learning_rate=lr)\n",
    "    # In the other compulsory assignments you will not submit notebooks with the training logs\n",
    "    histories.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax[0].set_title('Training Accuracy')\n",
    "ax[1].set_title('Validation Accuracy')\n",
    "\n",
    "for idx, neur in enumerate(hidden_layer_neurons):\n",
    "    history = histories[idx]\n",
    "    ax[0].plot(np.array(history['training_accuracy']), label='h=%i'%neur)\n",
    "    ax[1].plot(np.array(history['validation_accuracy']), label='h=%i'%neur)\n",
    "\n",
    "for axes in ax:\n",
    "    axes.set_xlabel('Epoch [%]')\n",
    "    axes.set_ylabel('Accuracy')\n",
    "    axes.grid('on')\n",
    "    axes.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Experiment with different activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_neurons   = 28*28 # Image dimensions\n",
    "hidden_layer_neurons   = 30\n",
    "outut_layer_neurons    = 10   # Number of digits to be recognized\n",
    "epochs                 = 15\n",
    "lr                     = 0.3\n",
    "activation_functions   = [sigmoid, tanh]\n",
    "d_activation_functions = [D_sigmoid, D_tanh]\n",
    "\n",
    "histories = []\n",
    "for idx in range(len(activation_functions)):\n",
    "    mnist_model = NeuralNetwork(input_layer_neurons, \n",
    "                                hidden_layer_neurons, \n",
    "                                outut_layer_neurons, \n",
    "                                activation_functions[idx], \n",
    "                                d_activation_functions[idx])\n",
    "    hist =  mnist_model.train(small_X_train, small_y_train, small_X_val, small_y_val, epochs=epochs, learning_rate=lr)\n",
    "    # In the other compulsory assignments you will not submit notebooks with the training logs\n",
    "    histories.append(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_functions = ['sigmoid', 'tanh']\n",
    "fig, ax = plt.subplots(1,2,figsize=(12,4))\n",
    "\n",
    "ax[0].set_title('Training Accuracy')\n",
    "ax[1].set_title('Validation Accuracy')\n",
    "\n",
    "for idx, af in enumerate(activation_functions):\n",
    "    history = histories[idx]\n",
    "    ax[0].plot(np.array(history['training_accuracy']), label='af=%s'%af)\n",
    "    ax[1].plot(np.array(history['validation_accuracy']), label='af=%s'%af)\n",
    "\n",
    "for axes in ax:\n",
    "    axes.set_xlabel('Epoch [%]')\n",
    "    axes.set_ylabel('Accuracy')\n",
    "    axes.grid('on')\n",
    "    axes.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Discuss the results and implement your own network\n",
    "Comment on your findings from tasks 4 - 6. Will this search suffice for exploring which combination of activation function, learning rate and hidden neuron count is the best? Why, or why not? \n",
    "\n",
    "Implement a network with the class you have designed with the hyperparameters of your choice and train it on the entire dataset. Feel free to explore multiple architectures, and evaluate them on `(X_test, y_test)`.\n",
    "Comment on how long it takes to train the network on the entire dataset compared to the small dataset. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of results from task 4-6:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code task 7:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_neurons  = 28*28 # Image dimensions\n",
    "hidden_layer_neurons  = \n",
    "outut_layer_neurons   = 10  # Number of digits to be recognized\n",
    "epochs                = \n",
    "lr                    = \n",
    "activation_function   = \n",
    "d_activation_function = \n",
    "\n",
    "mnist_model = NeuralNetwork(input_layer_neurons, hidden_layer_neurons, outut_layer_neurons, activation_function, d_activation_function)\n",
    "\n",
    "# Small dataset\n",
    "history = mnist_model.train(small_X_train, small_y_train, small_X_val, small_y_val, epochs=epochs, learning_rate=lr)\n",
    "test_accuracy = mnist_model.evaluate_accuracy(X_test, y_test)\n",
    "print('Test accuracy attained (small dataset):', test_accuracy, '%')\n",
    "mnist_model.plot_training()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion of task 7:**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "u_net",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
