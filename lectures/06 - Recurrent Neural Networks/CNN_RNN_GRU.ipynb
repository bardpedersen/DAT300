{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tacEPlLtWg7n"
      },
      "source": [
        "**This example provides a basic structure for combining CNN and RNN layer - This script performs the following steps:**\n",
        "\n",
        "    - Load the IMDB dataset, keeping only the top 10,000 most frequently occurring words in the training data.\n",
        "    Pad the sequences to ensure they all have the same length.\n",
        "    Build a sequential model with the following layers:\n",
        "       - An Embedding layer that turns positive integers (indexes) into dense vectors of fixed size.\n",
        "       - A Conv1D layer for the convolution operation that extracts features from the sequence data.\n",
        "       - A MaxPooling1D layer to reduce the spatial dimensions of the output from the convolutional layers.\n",
        "       - Another Conv1D layer for further feature extraction.\n",
        "       - An LSTM layer, which is a type of RNN, for analyzing the time-series data within the sequence.\n",
        "      -  A Dense layer with a sigmoid activation function for binary classification (positive or negative sentiment).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRKJ9oQ0WXZe",
        "outputId": "a1ff24a1-3b17-475b-9574-8720db0ae6cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "157/157 [==============================] - 129s 798ms/step - loss: 0.4551 - accuracy: 0.7631 - val_loss: 0.3039 - val_accuracy: 0.8742\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 123s 783ms/step - loss: 0.2086 - accuracy: 0.9238 - val_loss: 0.2991 - val_accuracy: 0.8780\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 121s 775ms/step - loss: 0.1236 - accuracy: 0.9587 - val_loss: 0.3686 - val_accuracy: 0.8640\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.0637 - accuracy: 0.9826 - val_loss: 0.4114 - val_accuracy: 0.8700\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 124s 793ms/step - loss: 0.0382 - accuracy: 0.9902 - val_loss: 0.4913 - val_accuracy: 0.8718\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 119s 758ms/step - loss: 0.0409 - accuracy: 0.9877 - val_loss: 0.4709 - val_accuracy: 0.8704\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 117s 745ms/step - loss: 0.0170 - accuracy: 0.9967 - val_loss: 0.5329 - val_accuracy: 0.8688\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 124s 793ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.5945 - val_accuracy: 0.8702\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 122s 776ms/step - loss: 0.0056 - accuracy: 0.9993 - val_loss: 0.6581 - val_accuracy: 0.8690\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 117s 744ms/step - loss: 0.0041 - accuracy: 0.9995 - val_loss: 0.6631 - val_accuracy: 0.8674\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.7522 - accuracy: 0.8548\n",
            "Test Accuracy: 0.8548399806022644\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, LSTM, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Set the number of words to keep based on frequency\n",
        "max_features = 10000\n",
        "\n",
        "# Sequence length to pad the outputs to\n",
        "max_len = 500\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gated Recurrent Units (GRU) **"
      ],
      "metadata": {
        "id": "7gSslXsgctxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GRU, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Set the number of words to keep based on frequency\n",
        "max_features = 10000\n",
        "\n",
        "# Sequence length to pad the outputs to\n",
        "max_len = 500\n",
        "\n",
        "# Load IMDB dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(Conv1D(32, 7, activation='relu'))\n",
        "model.add(GRU(32))  # Replacing LSTM with GRU\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAzheGsjdIlj",
        "outputId": "3255f44a-1713-4b33-91b4-7ed363f8df8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 121s 752ms/step - loss: 0.4699 - accuracy: 0.7531 - val_loss: 0.3139 - val_accuracy: 0.8696\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 116s 739ms/step - loss: 0.2079 - accuracy: 0.9202 - val_loss: 0.3000 - val_accuracy: 0.8796\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 115s 732ms/step - loss: 0.1177 - accuracy: 0.9597 - val_loss: 0.3728 - val_accuracy: 0.8638\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 115s 731ms/step - loss: 0.0503 - accuracy: 0.9863 - val_loss: 0.4557 - val_accuracy: 0.8624\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 116s 742ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 0.5352 - val_accuracy: 0.8666\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 120s 763ms/step - loss: 0.0087 - accuracy: 0.9982 - val_loss: 0.6165 - val_accuracy: 0.8668\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 115s 731ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.6572 - val_accuracy: 0.8596\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 116s 738ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.6558 - val_accuracy: 0.8592\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 116s 738ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.6343 - val_accuracy: 0.8730\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 119s 760ms/step - loss: 0.0083 - accuracy: 0.9975 - val_loss: 0.6331 - val_accuracy: 0.8700\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.7000 - accuracy: 0.8548\n",
            "Test Accuracy: 0.8548399806022644\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}