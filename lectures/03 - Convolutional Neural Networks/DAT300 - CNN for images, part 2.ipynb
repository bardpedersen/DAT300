{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Chapter 15 - Classifying Images with Deep Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization\n",
    "... is a research subject of its own. Some simple techniques are much used in Deep Learning:  \n",
    "- Momentum\n",
    "- Nesterov Momentum\n",
    "- Decay  \n",
    "  \n",
    "We will use code examples with simple linear regression adapted from https://github.com/jsphbtst/linear-regression-optimization-technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Linear regression using gradient descent\n",
    "# Parameters m: slope and b: intercept, cost function MSE\n",
    "def LinearRegression(X, y, epochs=1000, learning_rate=0.0001, activation=\"gradient_descent\", batch_size=256, \n",
    "                     mu=0.9, m_current=0, b_current=0):\n",
    "    N = float(len(y))\n",
    "    mini_batch_cost = []\n",
    "\n",
    "    if activation == \"gradient_descent\":\n",
    "        for i in range(epochs):\n",
    "            # Prediction\n",
    "            y_current = (m_current * X) + b_current            \n",
    "            \n",
    "            # Cost (MSE)\n",
    "            cost = sum([data**2 for data in (y-y_current)]) / N\n",
    "\n",
    "            # Derivative of cost function\n",
    "            m_gradient = -(2/N) * sum(X * (y - y_current))\n",
    "            b_gradient = -(2/N) * sum(y - y_current)\n",
    "\n",
    "            # Weight update\n",
    "            m_current = m_current - (learning_rate * m_gradient)\n",
    "            b_current = b_current - (learning_rate * b_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Stochastic / mini-batch alternative\n",
    "    elif activation == \"sgd\":\n",
    "        for i in range(epochs):\n",
    "            for j in range(0, int(N), batch_size):\n",
    "                # Prediction of current batch:\n",
    "                y_current = (m_current * X[j:j+batch_size]) + b_current\n",
    "\n",
    "                # Cost of current batch:\n",
    "                mini_batch_cost.append(sum([data**2 for data in (y[j:j+batch_size] - y_current)]) / N)\n",
    "\n",
    "                # Gradient of current batch\n",
    "                m_gradient = -(2/N) * sum(X[j:j+batch_size] * (y[j:j+batch_size] - y_current))\n",
    "                b_gradient = -(2/N) * sum(y[j:j+batch_size] - y_current)\n",
    "\n",
    "                # Weight updates\n",
    "                m_current = m_current - (learning_rate * m_gradient)\n",
    "                b_current = b_current - (learning_rate * b_gradient)\n",
    "\n",
    "            # Cost of epoch:\n",
    "            cost = sum(mini_batch_cost) / float(len(mini_batch_cost))\n",
    "            mini_batch_cost = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Momentum\n",
    "- The _classical momentum_ method (Polyak, 1964) is a technique for accelerating gradient descent that accumulates a velocity vector in directions of persistent reduction in the objective across iterations.\n",
    "    - \"Successful updates of weights are given an extra push.\"\n",
    "    - Momentum affects convergence most in the \"transient phase\", i.e. before fine tuning.\n",
    "- Think of a ball rolling down a hill\n",
    "    - When slope is negative in the direction of rolling, momentum increases\n",
    "    - When slope changes twists or is positive, momentum decreases\n",
    "$$\\nu_t = \\gamma \\nu_{t-1} + \\eta \\triangledown_\\theta J(\\theta),$$\n",
    "where $\\gamma$ is often around 0.9.\n",
    "- $\\nu_{t-1}$ is the memory from previous iterations, and $\\gamma$ controls how much is remembered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SGD / mini-batch with Momentum alternative\n",
    "    elif activation == \"momentum\":\n",
    "        for i in range(epochs):\n",
    "            for j in range(0, int(N), batch_size):\n",
    "                # Prediction of current batch:\n",
    "                y_current = m_current * X[j:j+batch_size] + b_current\n",
    "\n",
    "                # Cost of current batch:\n",
    "                mini_batch_cost.append(sum([data**2 for data in (y[j:j+batch_size] - y_current)]) / N)\n",
    "\n",
    "                # Gradient of current batch\n",
    "                m_gradient = -(2/N) * sum(X[j:j+batch_size] * (y[j:j+batch_size] - y_current))\n",
    "                b_gradient = -(2/N) * sum(y[j:j+batch_size] - y_current)\n",
    "\n",
    "                # Reset momentum for each epoch\n",
    "                if i == 0:\n",
    "                    v_m = 0\n",
    "                    v_b = 0\n",
    "\n",
    "                # Momentum update (previous accumulation + fresh gradient)\n",
    "                v_m = mu * v_m + learning_rate * m_gradient\n",
    "                v_b = mu * v_b + learning_rate * b_gradient\n",
    "\n",
    "                # Weight update\n",
    "                m_current = m_current - v_m\n",
    "                b_current = b_current - v_b        \n",
    "\n",
    "            # Cost of epoch:\n",
    "            cost = sum(mini_batch_cost) / float(len(mini_batch_cost))\n",
    "            mini_batch_cost = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nesterov momentum\n",
    "- Momentum may still be large near the optimum (imagine the ball again)\n",
    "- _Nesterov momentum_ adds a partial update of the gradient before adding momentum to the weight update.\n",
    "    - \"Successful updates of weights are given two extra pushes.\"\n",
    "    - First push is anticipatory, guessing a likely gradient update\n",
    "$$\\nu_t = \\gamma \\nu_{t-1} + \\eta \\triangledown_\\theta J(\\theta - \\gamma \\nu_{t-1})$$\n",
    "<img src=\"./images/Nesterov.jpeg\" alt=\"Momentum\" style=\"width: 500px;\"/>\n",
    "Source: http://cs231n.github.io/assets/nn3/nesterov.jpeg\n",
    "\n",
    "- Explanation and Python examples: https://towardsdatascience.com/a-bit-beyond-gradient-descent-mini-batch-momentum-and-some-dude-named-yuri-nesterov-a3640f9e496b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# SGD / mini-batch with Nesterov Momentum alternative\n",
    "    elif activation == \"nesterov\":\n",
    "        for i in range(epochs):\n",
    "            for j in range(0, int(N), batch_size):\n",
    "                # Prediction of current batch:\n",
    "                y_current = (m_current * X[j:j+batch_size]) + b_current\n",
    "\n",
    "                # Cost of current batch:\n",
    "                mini_batch_cost.append(sum([data**2 for data in (y[j:j+batch_size] - y_current)]) / N)\n",
    "\n",
    "                # Reset momentum for each epoch\n",
    "                if i == 0:\n",
    "                    v_m = 0\n",
    "                    v_b = 0\n",
    "\n",
    "                # Nesterov step\n",
    "                y_nesterov_m = (m_current - mu * v_m) * X[j:j+batch_size] + b_current\n",
    "                y_nesterov_b = (b_current - mu * v_b) * X[j:j+batch_size] + b_current\n",
    "\n",
    "                # Gradient of current batch with Nesterov step\n",
    "                m_gradient = -(2/N) * sum(X[j:j+batch_size] * (y[j:j+batch_size] - y_nesterov_m))\n",
    "                b_gradient = -(2/N) * sum(y[j:j+batch_size] - y_nesterov_b)\n",
    "\n",
    "                # Momentum update\n",
    "                v_m = mu * v_m + learning_rate * m_gradient\n",
    "                v_b = mu * v_b + learning_rate * b_gradient\n",
    "\n",
    "                # Weight update\n",
    "                m_current = m_current - v_m\n",
    "                b_current = b_current - v_b\n",
    "\n",
    "            cost = sum(mini_batch_cost) / float(len(mini_batch_cost))\n",
    "            mini_batch_cost = []\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"ERROR: Activation Function Not Found!\")\n",
    "\n",
    "    return m_current, b_current, cost            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decay\n",
    "- Where momentum accellerates in promising directions, decay reduces the learning rate.\n",
    "- Finetuning of models requires small steps.\n",
    "    - Shrink the learning rate gradually\n",
    "    - For instance $lr_{epoch ~ i+1} = lr_{epoch ~ i} \\times 0.99$\n",
    "- Too fast decay => never reach minimum\n",
    "- Decay with restart and more adaptive decays may be useful\n",
    "- [Reduce learning rate on plateaus](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizers\n",
    "- AdaGrad: Parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. \n",
    "    -  It adapts the learning rate to the parameters, performing smaller updates\n",
    "    - AdaDelta: Same idea, but in a local window of iterations (not accumulated over all updates).\n",
    "- RMSProp: Divide the gradient/learning rate for each weight by a running average of their recent, individual magnitudes - [Lecture by G.F.Hinton](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizers ctd.\n",
    "- Adam is a robust gradient-based optimization method inspired by RMSProp and AdaGrad,\n",
    "    - suited for nonconvex optimization and machine learning problems,\n",
    "    - choice of update step size derived from the running average of gradient moments.\n",
    "    - Used to have guaranteed convergence, but now reduced to practically always converges.\n",
    "- SGD with momentum also popular (no invidual learning rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/Optimization1.gif\" alt=\"Optimization\" style=\"width: 600px;\"/>  \n",
    "Image credit [Alec Radford](https://twitter.com/alecrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"./images/Optimization2.gif\" alt=\"Optimization\" style=\"width: 600px;\"/>  \n",
    "Image credit [Alec Radford](https://twitter.com/alecrad)  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "jupytext": {
   "cell_metadata_json": true,
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "rise": {
   "height": "95%",
   "scroll": false,
   "width": "95%"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
